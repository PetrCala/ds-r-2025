{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "R",
      "language": "R",
      "name": "ir"
    },
    "language_info": {
      "name": "R",
      "mimetype": "text/x-r-source",
      "file_extension": ".r"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Lesson 3: Advanced Data Cleaning & Text Handling in R\"\n",
        "author: \"Your Name\"\n",
        "date: \"Block Lecture 3\"\n",
        "---\n",
        "\n",
        "# Lesson 3 Notebook\n",
        "\n",
        "Welcome to **Lesson 3**! In the previous lessons, you learned:\n",
        "- **Lesson 1**: Reading in datasets, cleaning basics, and simple EDA.\n",
        "- **Lesson 2**: Reshaping, merging data frames, and a quick intro to SQL in R.\n",
        "\n",
        "Now, we’ll explore more **advanced data cleaning** techniques (especially for messy real-world data), **text processing** (since many journalism datasets include text), and wrap up with a **mini-project** to reinforce the entire workflow.\n",
        "\n",
        "## Topics\n",
        "1. Advanced Data Cleaning & Validation\n",
        "2. Handling and Cleaning Text Data\n",
        "3. Combining Data Wrangling and SQL\n",
        "4. Mini Workflow / Capstone Example\n",
        "5. Looking Ahead & Thesis Tips\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 1. Advanced Data Cleaning & Validation\n",
        "In Lesson 1, we touched on **missing values**, **renaming**, and **basic type conversion**. In real-life scenarios, data might have **inconsistent formats**, **typos**, **outliers**, or **invalid values**. Below are some approaches and packages that can help.\n",
        "\n",
        "### 1.1 Revisiting the `df` from Lesson 1 (or a new data set)\n",
        "We’ll assume you have a dataset (`df`) with potential issues:\n",
        "1. **String Inconsistencies**: different capitalization (`\"Marketing\"` vs. `\"marketing\"`).\n",
        "2. **Out-of-range values**: e.g., `Age` might be 150.\n",
        "3. **Whitespace or strange characters** in text fields.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load needed packages\n",
        "library(tidyverse)\n",
        "\n",
        "# Suppose we have a small example that simulates these issues\n",
        "df <- data.frame(\n",
        "  ID = 1:6,\n",
        "  Department = c(\"Marketing \", \"MARKETING\", \"Sales\", \"sales\", \"Sales \", \"IT \"),\n",
        "  Age = c(25, 200, 30, NA, 28, 45),\n",
        "  Comment = c(\"  Great Employee \", \"N/A\", \"excellent worker\", \"??\", \"n/a\", \"    \")\n",
        ")\n",
        "\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.2 Dealing with Inconsistent Text\n",
        "Sometimes you want to **trim whitespace**, convert to **lowercase** or **title case**, and replace placeholders (`\"N/A\"`, `\"n/a\"`) with actual `NA`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Stringr (part of tidyverse) is useful for text manipulations\n",
        "library(stringr)\n",
        "\n",
        "df_clean <- df %>%\n",
        "  mutate(\n",
        "    # Trim whitespace\n",
        "    Department = str_trim(Department),\n",
        "\n",
        "    # Convert to proper case or consistent case\n",
        "    Department = str_to_title(Department),\n",
        "\n",
        "    # Replace 'N/A' or 'n/a' or '??' with an actual NA in 'Comment'\n",
        "    Comment = na_if(Comment, \"N/A\"),\n",
        "    Comment = na_if(Comment, \"n/a\"),\n",
        "    Comment = na_if(Comment, \"??\"),\n",
        "\n",
        "    # Also trim whitespace for Comment\n",
        "    Comment = str_trim(Comment),\n",
        "\n",
        "    # If Comment is just empty string, treat as NA\n",
        "    Comment = ifelse(Comment == \"\", NA, Comment)\n",
        "  )\n",
        "\n",
        "df_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Note**: You could also create a small function that standardizes these steps (e.g., repeated calls to `na_if()`).\n",
        "\n",
        "### 1.3 Validating Numeric Ranges\n",
        "Next, suppose **Age** should never exceed 120. We can flag or fix out-of-range values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Flag unrealistic ages\n",
        "df_clean <- df_clean %>%\n",
        "  mutate(\n",
        "    Age = ifelse(Age > 120, NA, Age)  # treat out-of-range as NA\n",
        "  )\n",
        "\n",
        "df_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1.4 Additional Validation Packages\n",
        "- **`validate`**: a package that lets you define **validation rules**.\n",
        "- **`assertr`**: for creating **assertions** that your data meets certain conditions.\n",
        "\n",
        "> **Exercise**: Install and explore `validate` or `assertr` to define rules like `Age >= 0 & Age <= 120` or `Department` in a set of known departments.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 2. Handling and Cleaning Text Data\n",
        "\n",
        "As journalism students, you might deal with **text-heavy** data: articles, transcripts, tweets, etc. R provides packages like **`stringr`** and **`tidytext`** for text wrangling.\n",
        "\n",
        "### 2.1 Basic Stringr Operations\n",
        "We’ve seen **trim**, **to_lower**/**to_upper**. More advanced tasks:\n",
        "- `str_detect(text, pattern)`: Check if pattern is present.\n",
        "- `str_replace_all(text, pattern, replacement)`: Replace text with regex.\n",
        "- `str_split(text, pattern)`: Split text into substrings.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simple example: searching & replacing in a column\n",
        "text_data <- data.frame(\n",
        "  ID = 1:4,\n",
        "  Tweet = c(\n",
        "    \"This is #awesome!\",\n",
        "    \"Check out https://example.com for details\",\n",
        "    \"Breaking news: R is amazing???\",\n",
        "    \"Email me at test@example.com.\"  \n",
        "  )\n",
        ")\n",
        "\n",
        "text_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Example: Removing URLs or Email Addresses\n",
        "Let’s **remove** them or mask them in the text.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "text_clean <- text_data %>%\n",
        "  mutate(\n",
        "    # Remove URLs using a simple regex pattern\n",
        "    Tweet = str_replace_all(Tweet, \"http[^\"]+\", \"[LINK]\"),\n",
        "\n",
        "    # Remove email addresses\n",
        "    Tweet = str_replace_all(Tweet, \"[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Za-z]{2,}\", \"[EMAIL]\")\n",
        "  )\n",
        "\n",
        "text_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2.2 Tidytext Basics\n",
        "For **tokenization** (splitting text into words), **stopword** removal, or **word frequencies**, the `tidytext` package is invaluable. A quick demonstration:\n",
        "\n",
        "```r\n",
        "# install.packages(\"tidytext\") if needed\n",
        "library(tidytext)\n",
        "\n",
        "# We can unnest tokens to split each tweet into individual words\n",
        "text_tokens <- text_clean %>%\n",
        "  unnest_tokens(output = \"word\", input = \"Tweet\")\n",
        "\n",
        "# Count frequencies\n",
        "text_tokens %>% count(word, sort = TRUE)\n",
        "```\n",
        "\n",
        "> **Exercise**:\n",
        "1. Filter out **stopwords** (e.g., \"the\", \"is\", \"at\"), using `stop_words` from tidytext.\n",
        "2. Compare the top words used in your dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 3. Combining Data Wrangling and SQL\n",
        "By now, you’ve seen how to **reshape** (Lesson 2), **clean** (Lesson 1 & 3), and do quick **SQL** queries in R. Often, you’ll use **both**:\n",
        "- Use R’s `dplyr` or string functions for specialized wrangling.\n",
        "- Use SQL queries to filter or aggregate large data.\n",
        "\n",
        "### 3.1 Example: Clean + Query\n",
        "Below is a mini workflow example:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "library(sqldf)\n",
        "\n",
        "# 1. Suppose we have a data frame with messy text or numeric issues\n",
        "df_example <- data.frame(\n",
        "  user_id = 1:5,\n",
        "  status  = c(\"Active\", \"active\", \"inactive\", \"unknown\", \"ACTIVE\"),\n",
        "  score   = c(50, 80, 90, NA, 120)\n",
        ")\n",
        "\n",
        "# 2. Clean status text, set out-of-range score to NA\n",
        "df_example <- df_example %>%\n",
        "  mutate(\n",
        "    status = str_to_lower(status),\n",
        "    status = ifelse(status == \"unknown\", NA, status),\n",
        "    score  = ifelse(score > 100, NA, score)\n",
        "  )\n",
        "\n",
        "# 3. Query using sqldf\n",
        "sqldf(\"SELECT user_id, status, score FROM df_example WHERE score >= 60\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "That’s a simplified illustration. In practice, you might have multiple data frames with complex relationships, or you might prefer to store them in a **SQLite** database. The key is: **R** + **SQL** can cooperate seamlessly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 4. Mini Workflow / Capstone Example\n",
        "Now let’s outline a **mini-project** that ties everything together. We’ll do it conceptually here, but encourage you to apply it to **real** data you’re investigating for your thesis or class project.\n",
        "\n",
        "### 4.1 Scenario\n",
        "You have a dataset of **news articles** with the following columns:\n",
        "- `article_id`, `title`, `author`, `content`, `published_date`, `category`, etc.\n",
        "Some of the article content may contain missing text, or placeholders. You also have a **lookup** table that maps `category` codes to more descriptive labels (e.g., `POL = Politics`, `ENT = Entertainment`).\n",
        "\n",
        "### 4.2 Steps\n",
        "1. **Import** the main article dataset (`articles.csv`) and the category lookup file (`category_lookup.csv`).\n",
        "2. **Clean** the article text: remove or replace certain patterns (URLs, email addresses), standardize capitalization in `title`, handle missing or placeholder content.\n",
        "3. **Join** the main dataset with `category_lookup` to get descriptive names.\n",
        "4. **Explore** text frequencies or word counts (with `tidytext`) to see which terms appear most often in each category.\n",
        "5. **Optional**: Store the cleaned dataset in a local SQLite database, run a few **SQL** queries for practice.\n",
        "6. **Visualize** the top words or categories in a bar chart or word cloud.\n",
        "7. **Export** the final cleaned dataset.\n",
        "\n",
        "### 4.3 Example Code Snippet (Pseudocode)\n",
        "```r\n",
        "# 1. Import\n",
        "articles <- read_csv(\"articles.csv\")\n",
        "lookup   <- read_csv(\"category_lookup.csv\")\n",
        "\n",
        "# 2. Clean text\n",
        "articles <- articles %>%\n",
        "  mutate(\n",
        "    title   = str_to_title(title),\n",
        "    content = str_replace_all(content, \"http[^\"]+\", \"[LINK]\"),\n",
        "    content = str_trim(content),\n",
        "    content = ifelse(content == \"\", NA, content)\n",
        "  )\n",
        "\n",
        "# 3. Join with category lookup\n",
        "articles_joined <- left_join(articles, lookup, by = \"category\")\n",
        "\n",
        "# 4. Explore text frequencies\n",
        "library(tidytext)\n",
        "tokenized <- articles_joined %>%\n",
        "  unnest_tokens(word, content)\n",
        "\n",
        "# 5. Summaries / Queries\n",
        "library(sqldf)\n",
        "sqldf(\"SELECT descriptive_category, COUNT(*) as count FROM tokenized GROUP BY descriptive_category\")\n",
        "\n",
        "# 6. Visualization\n",
        "tokenized %>%\n",
        "  count(word, sort = TRUE) %>%\n",
        "  filter(n > 50) %>%\n",
        "  ggplot(aes(x = reorder(word, n), y = n)) +\n",
        "  geom_col() +\n",
        "  coord_flip() +\n",
        "  labs(title = \"Most Common Words\", x = \"Word\", y = \"Count\")\n",
        "\n",
        "# 7. Export\n",
        "write_csv(articles_joined, \"articles_cleaned.csv\")\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Tips**:\n",
        "1. Show how you cleaned the data in your project or thesis: create an **Appendix** with code.\n",
        "2. Keep track of each step so it’s reproducible.\n",
        "3. Use Git or version control if possible to track changes.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 5. Looking Ahead & Thesis Tips\n",
        "1. **Text Data**: Journalism students often face unstructured text. Keep practicing `stringr` and `tidytext`.\n",
        "2. **SQL**: If your data is large, consider a database approach. Use R’s **DBI** or **sqldf** for more complex queries.\n",
        "3. **Validation**: Tools like `validate` or `assertr` help ensure data meets certain rules. This is crucial for **fact-checking** or ensuring consistent data quality.\n",
        "4. **Reproducibility**: Try using **R Markdown** or **Quarto** to produce **reproducible reports**. Insert your code and outputs into a single document for your thesis.\n",
        "5. **Version Control**: If you collaborate, store your project in a Git repository or a shared platform so you can revert changes and track your progress.\n",
        "\n",
        "Congratulations on making it this far! You now have a robust toolkit for **importing**, **cleaning**, **reshaping**, **merging**, **text processing**, and even **SQL querying** in R. These skills will serve you well in data-driven journalism and beyond.\n",
        "\n",
        "# End of Lesson 3\n"
      ]
    }
  ]
}
