{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07b9b4f3",
   "metadata": {},
   "source": [
    "# Part 2, Lesson 3: APIs, Web Scraping & JSON Handling in R\n",
    "**Author:** Your Name  \n",
    "**Date:** Block Lecture (4 hours)\n",
    "\n",
    "# Part 2 – Lesson 3: APIs, Web Scraping, & JSON\n",
    "\n",
    "Welcome to the **third 4-hour session** of Part 2! This lesson explores how to **obtain** data from the web—either through **public APIs** or by **scraping** HTML pages—and handle **JSON** or other semi-structured formats in R.\n",
    "\n",
    "## Topics\n",
    "1. **Recap & Motivation**\n",
    "2. **Accessing APIs with R**\n",
    "3. **JSON Basics & Parsing**\n",
    "4. **Web Scraping** with `rvest`\n",
    "5. **Storing Scraped Data in a Database**\n",
    "6. **Practical Exercises & Best Practices**\n",
    "\n",
    "> **Outcome**: By the end, you should be comfortable making **HTTP requests**, parsing **JSON** or **HTML** results, and saving that data for further analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fdd9178",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Recap & Motivation\n",
    "**Why** focus on APIs or scraping?\n",
    "- Many **public datasets** are available via REST APIs (e.g., government portals, social media, weather).\n",
    "- Some data isn’t provided in a convenient format, so you might need to **scrape** it from websites.\n",
    "- Journalists often need to gather info from multiple sources quickly.\n",
    "\n",
    "### Recap & Q&A\n",
    "- Questions from previous lessons about **SQL**, **window functions**, or **database** usage?\n",
    "- Experience or challenges retrieving external data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc64df",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Accessing APIs with R\n",
    "### 2.1 HTTP & REST Basics\n",
    "Most modern data APIs use **HTTP** methods like `GET`, `POST`, etc. A typical request might look like:\n",
    "```\n",
    "GET https://api.openweathermap.org/data/2.5/weather?q=London&appid=YOUR_API_KEY\n",
    "```\n",
    "The server returns **JSON** (or XML), which we parse.\n",
    "\n",
    "### 2.2 Using `httr` or `curl`\n",
    "R packages for making requests:\n",
    "- **`httr`**: High-level functions for GET/POST.\n",
    "- **`curl`**: Lower-level approach.\n",
    "- **`jsonlite`**: Often used for parsing JSON.\n",
    "\n",
    "Let’s demonstrate a **public** API example. (We’ll use a mock or test endpoint if a real key is needed.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02537915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install if needed:\n",
    "# install.packages(\"httr\")\n",
    "# install.packages(\"jsonlite\")\n",
    "\n",
    "library(httr)\n",
    "library(jsonlite)\n",
    "\n",
    "# Example: GitHub's public API for user data (no API key needed)\n",
    "url <- \"https://api.github.com/users/hadley\"\n",
    "\n",
    "res <- GET(url)  # Make GET request\n",
    "\n",
    "# Check status\n",
    "status_code(res)\n",
    "\n",
    "content_raw <- content(res, as = \"text\")  # Get raw text\n",
    "content_json <- fromJSON(content_raw)\n",
    "\n",
    "str(content_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdea653",
   "metadata": {},
   "source": [
    "The `content_json` variable is now a **list** in R containing user details from GitHub’s API. We can extract fields like `content_json$login`, `content_json$name`, etc.\n",
    "\n",
    "### 2.3 Handling API Keys\n",
    "Some APIs require an **API key** or **OAuth** token. Typically:\n",
    "```r\n",
    "res <- GET(\n",
    "  \"https://api.example.com/data\",\n",
    "  add_headers(Authorization = paste(\"Bearer\", MY_API_TOKEN))\n",
    ")\n",
    "```\n",
    "You’d store `MY_API_TOKEN` in an **environment variable** or a secure file, not directly in your script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94761f0b",
   "metadata": {},
   "source": [
    "### 2.4 Rate Limits & Pagination\n",
    "Many APIs have **rate limits** (e.g., 60 requests/hour). For large data, you might need to **paginate** or request data in **batches**.\n",
    "```r\n",
    "# Pseudocode for paginated results\n",
    "page = 1\n",
    "results_all = list()\n",
    "repeat {\n",
    "  res <- GET(paste0(\"https://api.example.com/data?page=\", page))\n",
    "  data_page <- fromJSON(content(res, \"text\"))\n",
    "  if (length(data_page) == 0) break  # no more data\n",
    "  results_all[[page]] <- data_page\n",
    "  page <- page + 1\n",
    "}\n",
    "final_data <- bind_rows(results_all)\n",
    "```\n",
    "Always read the **API docs** for how to navigate or handle large queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e61c1b",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. JSON Basics & Parsing\n",
    "**JSON** (JavaScript Object Notation) is a lightweight format for data exchange, frequently used by APIs. R can parse JSON with **`jsonlite`** or **`rjson`**. We’ll use `jsonlite`.\n",
    "\n",
    "### 3.1 Parsing JSON from a String\n",
    "```r\n",
    "library(jsonlite)\n",
    "json_text <- '{\"name\": \"Alice\", \"age\": 30, \"pets\": [\"dog\", \"cat\"]}'\n",
    "obj <- fromJSON(json_text)\n",
    "str(obj)\n",
    "# $ name: chr \"Alice\"\n",
    "# $ age : num 30\n",
    "# $ pets: chr [1:2] \"dog\" \"cat\"\n",
    "```\n",
    "We can **navigate** `obj` like a list: `obj$name` or `obj$pets`.\n",
    "\n",
    "### 3.2 Converting R Objects to JSON\n",
    "```r\n",
    "my_list <- list(city = \"Berlin\", population = 3.6, landmarks = c(\"Brandenburg Gate\", \"Reichstag\"))\n",
    "toJSON(my_list, pretty = TRUE)\n",
    "```\n",
    "Useful if you need to send data **to** an API or store config in JSON.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fec3c67",
   "metadata": {},
   "source": [
    "### 3.3 Flattening Nested JSON\n",
    "APIs often return **nested** structures. The function **`fromJSON(..., flatten = TRUE)`** can help produce a more tabular structure. If that fails, you can **manually** navigate and reformat.\n",
    "\n",
    "> **Exercise**: Use a public JSON API, parse the data, and try to flatten it into a dataframe. Then, store that data in a database table (tying in your **SQL** skills from previous lessons).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0492e",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Web Scraping with `rvest`\n",
    "When an API doesn’t exist, or the data is behind an **HTML** page, we can **scrape** it. **`rvest`** (from the tidyverse) is a top choice.\n",
    "\n",
    "### 4.1 Basic Workflow\n",
    "1. **Read** the webpage’s HTML.\n",
    "2. Identify **CSS selectors** or **XPath** that point to the desired elements.\n",
    "3. Extract text or attributes.\n",
    "4. Clean & structure the data.\n",
    "\n",
    "### 4.2 Example\n",
    "We’ll scrape a small example from [example.com] or a placeholder site. (For real scraping, pick an actual site that allows it and check **robots.txt** / terms of service!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd542e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install.packages(\"rvest\") if needed\n",
    "library(rvest)\n",
    "\n",
    "# Let's use a sample page for demonstration:\n",
    "url2 <- \"https://example.com/\"\n",
    "\n",
    "# 1) Read HTML\n",
    "page <- read_html(url2)\n",
    "\n",
    "# 2) Extract the <h1> tag text\n",
    "h1_text <- page %>% html_element(\"h1\") %>% html_text()\n",
    "h1_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9f7f9",
   "metadata": {},
   "source": [
    "You’d follow the same approach for **tables**, **lists**, or **div** elements. For example, if a page has a table:\n",
    "```r\n",
    "table_data <- page %>% \n",
    "  html_element(\"table\") %>% \n",
    "  html_table()\n",
    "```\n",
    "This transforms the HTML table into a dataframe (assuming it’s well-structured HTML).\n",
    "\n",
    "### 4.3 Handling Pagination or Multiple Pages\n",
    "Similar to an API, you might have multiple pages to scrape. You can loop over URLs:\n",
    "```r\n",
    "pages <- c(\"page1.html\", \"page2.html\")\n",
    "all_data <- list()\n",
    "for (i in seq_along(pages)) {\n",
    "  pg <- read_html(pages[i])\n",
    "  data_extracted <- pg %>% ...\n",
    "  all_data[[i]] <- data_extracted\n",
    "}\n",
    "final <- bind_rows(all_data)\n",
    "```\n",
    "Again, **respect** the site’s usage policy, consider **delays** between requests, and handle **error checking** for missing pages.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55210d21",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Storing Scraped Data in a Database\n",
    "Tie in your **SQL** knowledge. Once you have the final dataframe from the API or web page, you can store it in a database (SQLite, MySQL, etc.).\n",
    "```r\n",
    "library(DBI)\n",
    "con <- dbConnect(RSQLite::SQLite(), \"scraped_data.sqlite\")\n",
    "\n",
    "# Suppose we have final_data from an API or scraped table\n",
    "dbWriteTable(con, \"my_table\", final_data)\n",
    "```\n",
    "Then you can run queries, merges, or further analysis on that table—**closing the loop** on your data pipeline.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27933d28",
   "metadata": {},
   "source": [
    "### A Mini ETL Example\n",
    "1. **Extract** from an API or website with `httr`/`rvest`.\n",
    "2. **Transform** the JSON/HTML structures into clean data.\n",
    "3. **Load** the data into a database with `dbWriteTable`.\n",
    "4. Use SQL or `dplyr` to do further summarization or linking to other datasets.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207a76cb",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Practical Exercises & Best Practices\n",
    "Here are some suggestions for hands-on **exercises**:\n",
    "\n",
    "1. **API**: Pick a public API (GitHub, OpenWeatherMap, etc.). Make a **GET** request, parse the JSON, flatten it, and store the relevant fields in a local SQLite database.\n",
    "2. **Web Scraping**: Identify a website that lists data in a table or items. Use `rvest` to scrape it into a dataframe. Clean up the text (remove whitespace, parse numbers), and store it in your DB.\n",
    "3. **Combine**: If you have local CSVs or a second database table, do a **join** or **merge** to add context.\n",
    "4. **Rate Limits / Politeness**: If you scrape multiple pages, insert a short delay with `Sys.sleep(1)` to avoid hammering the server.\n",
    "\n",
    "### Best Practices\n",
    "- Check **robots.txt** or the site’s terms to ensure scraping is allowed.\n",
    "- Use **caching** if you repeatedly fetch the same data.\n",
    "- Don’t store **API keys** in your code repository. Use environment variables or `.Renviron`.\n",
    "- For large-scale scraping, consider specialized tools or a queue-based approach.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce089a4",
   "metadata": {},
   "source": [
    "---\n",
    "## Wrap-Up & Next Steps\n",
    "\n",
    "In this **Part 2, Lesson 3**, you learned:\n",
    "- **Accessing APIs** in R via `httr`, handling JSON with `jsonlite`.\n",
    "- **Web scraping** basics using `rvest`.\n",
    "- Converting messy nested data into R data frames.\n",
    "- Saving the final results into your **database** or local files for further analysis.\n",
    "\n",
    "### What’s Next?\n",
    "1. **Expand** your scraping or API usage: handle pagination, error handling, or session-based logins.\n",
    "2. Integrate **SQL** window functions or joins on your newly scraped data.\n",
    "3. Build a **Shiny** app or custom script to automate data ingestion from an API or website.\n",
    "\n",
    "### Additional Resources\n",
    "- [**rvest** documentation](https://rvest.tidyverse.org/)\n",
    "- [**httr** package vignettes](https://httr.r-lib.org/articles/)\n",
    "- [**jsonlite** reference](https://github.com/jeroen/jsonlite)\n",
    "- [**W3C** docs on JSON-LD, if you deal with structured web data (Schema.org)](https://json-ld.org/)\n",
    "- [**Scraping guidance** by NICAR or other journalism orgs] for legal/ethical considerations.\n",
    "\n",
    "This completes your introduction to **APIs, web scraping, and JSON** in R. Happy data hunting!\n",
    "\n",
    "# End of Part 2, Lesson 3\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
